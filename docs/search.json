[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Regression",
    "section": "",
    "text": "“Everything is related to everything else, but near things are more related than distant things.”"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Spatial Regression",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe assume that readers have a basic proficiency in R, data analysis, and introductory probability theory/statistics. While each topic is presented in an applied manner, and the overall mathematical complexity is low, a basic background in statistics is necessary to understand the fundamental concepts discussed throughout this guide. At the most basic level, a working knowledge of the Gauss-Markov assumptions, statistical inference, and programming with library(tidyverse) is needed. Further, some basic experience working with spatial data, library(sf), and making maps would be extremely helpful.\nBefore following along with the applied example, be sure to install and load the packages below:\n\n## The Following Packages Are Used for the Analysis\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readr)\nlibrary(sf)\nlibrary(spdep)\nlibrary(stargazer)\nlibrary(spatialreg)\nlibrary(patchwork)\nlibrary(GWmodel)\nlibrary(magrittr)\n\nimage source"
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "Basics of Geospatial Data",
    "section": "",
    "text": "In this short introduction, we offer some motivations and cover basic geospatial analysis in R. To showcase the power of geospatial analysis for applied researchers, we rely on an application of exploring poverty and education in New York. By the end of this chapter, you should be comfortable finding, cleaning, and visualizing the spatial data that you are interested in. Only when you have mastered these concepts, can you confidently begin to model spatial relationships."
  },
  {
    "objectID": "chapter1.html#motivation",
    "href": "chapter1.html#motivation",
    "title": "Basics of Geospatial Data",
    "section": "Motivation",
    "text": "Motivation\nGeospatial data is ubiquitous in the social sciences and tools to systematically analyze spatial variables and spatial relationships should be a part of every data scientist and social scientist’s toolkit. From visualizing election returns in American politics to depicting different countries’ gross domestic products (GDP), the ability to make a map is immensely useful. Every article concerning state and local politics in political science will usually showcase a map of states or counties at some point in order to effectively illustrate the distributions of the variables important to the analysis. Macroeconomists and comparative political scientists conducting cross-national studies similarly use maps to illustrate the spatial distribution of their key variables. Even when researchers are studying relationships that are not inherently spatial, including a map is a powerful visualization technique.\nIf the unit of analysis is a spatial unit - such as country-level, state-level, county-level or topographical - maps are perhaps the best way to visualize the distribution of your variables of interest across space. Further, sociologists, economists, and epidemiologists who are interested in examining crime and health data will find particular advantages in being able to map crime rates, the prevalence of a disease, or the health outcomes in different areas of a town or city. Public administration and policy researchers who want to make recommendations to decision-makers about where to build police stations, hospitals, fire departments, parks, or concentrate funding to targeted areas can utilize a well-made map and spatial regression techniques to draw robust conclusions. Perhaps most importantly, researchers who want their work to be public-facing and read by people without technical expertise could find more success by displaying their results and analysis through maps than through tables or even other, non-map, data visualizations.\nGeospatial data is also extremely common and readily available, with easy access to US Census data through library(tidycensus). Shapefiles and spatial data are also available through many cities, states, and government agencies’ data repositories. The use of application programming interfaces (APIs) has made accessing the wealth of spatial data collected by the world’s governments and businesses extremely easy. library(sf) also works well with library(dplyr) and library(ggplot2) to make creating attractive maps just as easy as accessing spatial data, overcoming the inherent challenges with spatial data being multidimensional.\nAs a data scientist or social scientist thinking through their research design, if the variables or relationship that is being modeled is spatial, then making a basic map and thinking through the spatial distribution of important variables should be the researchers first step. Where do researchers go after the creation of their map, though? The goal of this treatment on spatial regression is to equip applied researchers with the tools and theoretical background needed to go a step beyond mere visualization and discussion of geospatial data to estimating geospatial regression models. Further, readers will learn when spatial autocorrelation could render standard errors from OLS models incorrect and how to detect spatial autocorrelation in spatial data. Essentially, whenever a researcher is modeling a relationship that has spatial autocorrelation, they should use one of the techniques described in this work to purge autocorrelation from the error term of their models. After reading this guide, researchers will know when to use spatial regression methods and how to visualize the results of spatial regression models."
  },
  {
    "objectID": "chapter1.html#two-types-of-spatial-data",
    "href": "chapter1.html#two-types-of-spatial-data",
    "title": "Basics of Geospatial Data",
    "section": "Two Types of Spatial Data",
    "text": "Two Types of Spatial Data\nSpatial Data provides information on ‘something’ with respect to its location. This information is called attribute. In itself, attribute is not spatial data. In combination with information about it’s position it becomes spatial data.\nTemperature at a point when combined with latitude or longitude, height of a person in a room with respect to distance from two walls, etc., are all examples of attributes with information of location becoming spatial data.\nPurely spatial information of entities are represented by data models. Basic types of data models that we come across in geospatial analysis are the following:\n1. Point - A single point.\n2. Line - A stream of ordered points connected by straight line segments.\n3. Polygon - A set of lines forming a closed loop enclosing an area.\n4. Grid - A collection of points or rectangular cells, organised in regular lattice.\nThe first three are known as Vector Data, and represent entities as exactly as possible while the the last one is Raster Data. Raster data is used to represent continuous values by breaking them into finer tessellations (Bivand et al. 2008).\nSame data can be expressed as both raster and vector data types. When in raster form, data are stored as values of pixels in a matrix of cells. When in vector form, data are stored as value attributes in association with a set of coordinates.\nShapefiles are an example of vector data and and satellite images are raster data.\n\n\n\n\n\nPhoto by: GISlounge"
  },
  {
    "objectID": "chapter1.html#mapping-inr",
    "href": "chapter1.html#mapping-inr",
    "title": "Basics of Geospatial Data",
    "section": "Mapping inR",
    "text": "Mapping inR\nNow that we have covered some basic motivations and presented the two types of spatial data, it should be pretty clear that spatial analysis skills are essential for data scientists and social scientists. To further illustrate the benefits of knowing how to analyze geospatial data, we will discuss an applied example that introduces the analysis presented in the fourth chapter.\nAs mentioned above, the first step in any geospatial analysis is creating a map of the variables of interest. In order to start a spatial analysis, we will first need to collect spatial data that can be used to answer our research question. Our broad research question for our guide is: does a college education reduce poverty? Once a researcher has their research question, finding data can be done with the get_acs() function from library(tidycensus) or by accessing spatial data in a data repository. The goal is to acquire measures of the variables that we are interested in and the geometry data needed to map and analyze spatial units. Shapefiles contain this information. You can find this geometry information, in the form of shapefiles, from many governmental and private businesses’ data portals. Using library(sf) makes creating maps easy. This guide focuses mainly on spatial regression, so its use of library(sf) is limited. More help with using library(sf) can be found here and here.\nWe start our analysis by reading our data into R with st_read(). The example code below utilizes data from the American Community Survey’s 2008-2012 results. This data can be found here and our analysis is adapted from DGES (2022). We use st_set_crs() to set the CRS ID to 4326. The CRS, or coordinate reference system, ID tells R where the points and lines that make up our data’s spatial geometry are located in geographic space. The CRS ID also tells R what method should be used to flatten or project spatial units into geographic space. Below we use library(sf) and library(dplyr) together to read in the data and set the CRS ID to 4326. The clean_names() function from library(janitor) makes all of the variable names lowercase and replaces spaces with underscores.\n\n## Packages Needed\nlibrary(tidyverse)\nlibrary(sf)\n\n## Reading in shapefile\nny_shape <- \n  st_read(\"data/nyctract_acs/NYC_Tract_ACS2008_12.shp\") %>%\n  st_set_crs(value = 4326) %>%\n  janitor::clean_names()\n\nReading layer `NYC_Tract_ACS2008_12' from data source \n  `C:\\Users\\17176\\Documents\\Data Science\\ppol670-final-book\\data\\nyctract_acs\\NYC_Tract_ACS2008_12.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2166 features and 113 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49612 xmax: -73.70001 ymax: 40.91553\nCRS:           NA\n\n\nWe now have our dataset read into R. We can use dim() or look in the top right pane to see that our dataset has 2166 observations of 114 variables covering demographic and economic information at the census tract level for all of New York’s five boroughs. This is obviously quite a lot of variables and numerous questions of interest for social scientists and policy researchers can be answered with this dataset. For tractability, we will limit our analysis to a couple of variables in only one of New York’s boroughs; we will visualize poverty in Brooklyn, New York City’s most populous borough.\nBefore starting any analysis, researchers will probably need to go through the sometimes lengthy and difficult process of data cleaning and wrangling. During the data cleaning and wrangling stage, researchers can remove NA values they do not want; create new variables that better capture your intended research design; and format your data in the most convenient manner for your analysis. For this example, much of the cleaning of the raw ACS data has been done by the Department of Geography and Environmental Science at Hunter College in New York, New York. They have also included the weighted variables which account for the survey’s complex sampling procedure. The only wrangling necessary is to turn the poverty variable, currently measured as the total number of people living below the poverty line in a census tract, into a poverty rate variable that captures the proportion of a census tract’s residents living below the poverty line. The code below creates this variable and uses geom_sf() to create a map of poverty rates by the census tract in Brooklyn. We also filter to exclude relatively underpopulated census tracts. Since we plan to utilize the proportion of a census tract’s residents with a bachelor degree to predict the census tract’s poverty rate, we also create a proportion of residents with a bachelor’s degree variable.\n\n## Poverty Map\nbrook_shape <- ny_shape %>%\n  filter(boroname == \"Brooklyn\", poptot>200) %>%\n  mutate(poverty_rate=(poor/poptot),\n         bach_rate = (onlybachel/poptot))\n\nm1 <- brook_shape %>%\n  ggplot(aes(fill=poverty_rate)) +\n  geom_sf(color=\"white\", lwd=.1) +\n  scale_fill_gradient(guide=\"colorbar\", na.value=\"white\") + \n  theme_void() +\n  scale_fill_gradient2(midpoint = mean(brook_shape$poverty_rate)) +\n  labs(title=\"Poverty Rate in Brooklyn\",\n       fill=NULL, caption = \"Data Source: 2012 ACS\")\nm1\n\n\n\n\nWe can see from the map above that the poverty rate is relatively low across much of Brooklyn, but there are several pockets throughout the borough that exhibit high levels of poverty. These clusters are indicated by the darker purple tracts. The darker the purple, the higher the poverty rate, and the darker the red, the lower the poverty rate. High and low values are determined as a comparison to the mean level of poverty in Brooklyn. Utilizing the scale_fill_gradient2() function with the midpoint argument, we can see that the white tracts represent the average level of poverty throughout Brooklyn.\nThe map shows that the southern tip and the northeast section of Brooklyn have the highest rates of poverty in the borough. Much of the southeastern and the northwestern census tracts seem to have the lowest levels of poverty. These observations are a good first start in analyzing the distribution of poverty across Brooklyn, and, if we only wanted to conduct a descriptive study, we could perhaps end here. Our goal, however, is to explore the relationship between college education and poverty rates. To do this, we need to connect a census tract’s rate of poverty with its rate of college education. A logical next step is to map the proportion of a census tract’s residents that graduated college. The map, and the code to produce the map, is below:\n\n## Bachelor's Degree Map\nm2 <- brook_shape %>%\n  ggplot(aes(fill=bach_rate)) +\n  geom_sf(color=\"white\", lwd=.1) +\n  scale_fill_gradient(guide=\"colorbar\", na.value=\"white\") + \n  theme_void() +\n  scale_fill_gradient2(midpoint = mean(brook_shape$bach_rate)) +\n  labs(title=\"Percent Bachelor's Degree in Brooklyn\",\n       fill=NULL, caption = \"Data Source: 2012 ACS\")\nm2\n\n\n\n\nMapping the proportion of Brooklyn residents with a bachelor’s degree in each census tract shows that the northwest part of Brooklyn is highly educated, and there are some clustered census tracts of high bachelor’s degree attainment throughout the city. The majority of the census tracts in Brooklyn, however, have below the average rate of college educated residents.\nNow that we have our maps, we can display them side-by-side using library(patchwork) to try and draw informal inferences from the distributions of poverty rates and bachelor’s degree holders.\n\n## Presenting Maps side-by-side\nlibrary(patchwork)\nm1 + m2\n\n\n\n\nWe can see that the areas with high levels of poverty appear to have lower rates of residents holding bachelor’s degrees. For example, the northwest cluster of high college education rate census tracts is also a cluster of low poverty rate census tracts. The eastern cluster of low, the lowest consistent cluster in the borough, college degree attainment census tracts is also a cluster of high poverty census tracts. Looking at these maps, there does seem to be a pattern between low education and high poverty areas. This is generally supportive of the notion that higher levels of education lead to poverty reduction, but we would want a more formal hypothesis test before drawing any firm conclusions. This visual analysis would also only be able to identity a correlation, at best, without employing the more rigorous techniques of causal inference. There further appears to be clusters of poverty and clusters of higher educated census tracts evident in the maps which indicate some degree of spatial autocorrelation, but we cannot be sure without a formal test.\nFrom these maps, we cannot infer a causal effect, prediction, or any rigorous parameter estimate. Any observation based conclusion will be purely conjecture. While this visualization may be important for presenting results in the our final report or helping to understand how variables are distributed across space, our treatment on spatial regression will provide researchers with the necessary tools to take geospatial analysis one step further and estimate causal effects and predictions based on spatial relationships. While visualizing spatial data in the form of maps is one of the benefits of working with geospatial data, the richness of the data can be further utilized in more advanced statistical techniques.\nIn sum, there are two main reasons to learn spatial regression tools: (1) to be able to move beyond mapping variables to estimating complex spatial relationships and (2) to know how to create spatial models that purge spatial autocorrelation from the error term."
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "Congratulations, you have defined a research question, developed a theory, collected spatial data, visualized your variables of interest with a map, and now you are ready to estimate a causal effect or make some prediction about a spatial relationship. The logical first step is to estimate the workhorse model of social science: ordinary least squares. If your variables contain spatial autocorrelation, which is common in most spatial relationships, the OLS equation for standard errors will not be appropriate, though. This means that your standard errors will be incorrect, resulting in incorrect statistical inference. The same phenomenon is seen with time series data where methods such as \\(\\rho\\)-transforming the data or estimating Prais-Winston or Newey-West standard errors is needed to draw accurate inferences. Similar methods can be used to purge spatial autocorrelation from our models. While in time series data, autocorrelation refers to the residuals in time period \\(t_{-1}\\) being correlated with the residuals in time period \\(t\\) (Bailey 2021), in spatial data, autocorrelation refers to the residuals in unit \\(i\\) being correlated with the residuals in unit \\(j\\).\nIn this chapter, we move away from the applied example of education and poverty in Brooklyn in order to introduce one of the most important concepts related to spatial regression: spatial autocorrelation. We start by presenting the theoretical concept of spatial autocorrelation, show how to detect it, and run a simulation to illustrate it. We then introduce a naive OLS model of a spatial relationship and discuss how spatial autocorrelation violates the Gauss-Markov assumption that errors are not correlated with each other. By the end of this chapter you should be comfortable with the concept of spatial autocorrelation, how to detect it, and why it violates the Gauss-Markov assumptions."
  },
  {
    "objectID": "chapter2.html#detecting-spatial-autocorrelation",
    "href": "chapter2.html#detecting-spatial-autocorrelation",
    "title": "Spatial Autocorrelation",
    "section": "Detecting Spatial Autocorrelation",
    "text": "Detecting Spatial Autocorrelation\nHere, we present the concepts of global and local spatial autocorrelation and detail formal tests to detect them. We also introduce the core packages for working with spatial regression models in R: library(spdep) and library(spatialreg). We will apply the tests discussed in this chapter to our Brooklyn example in chapter four.\n\nGlobal vs. Local Spatial Autocorrelation\nSpatial autocorrelation measures the correlation of a variable with itself across space, similar to how serial autocorrelation measures the correlation of a variable with itself across time. Variables that are spatially autocorrelated can either be positively or negatively autocorrelated. If spatial autocorrelation is positive, locations close together have similar, approaching the same, values. If spatial autocorrelation is negative, locations close together have more dissimilar, approaching opposite, values.\nThere are two levels of spatial autocorrelation (DGES 2022 & Darmofal 2015): global and local. Global spatial autocorrelation quantifies the degree to which areas that are close by tend to be more alike (Darmofal 2015). This is generally what we mean when we use the term spatial autocorrelation. When we discuss global - as opposed to local autocorrelation - we are talking about the degree of clustering, not on the specific locations of possible clusters. In other words, global autocorrelation is how similar units are, on average, to their neighbors. Detecting and accounting for global autocorrelation is the first step towards accurate statistical inference in spatial regression models.\nIn contrast to global autocorrelation, local autocorrelation, also called local indicators of spatial association, or LISA, tells us where clustering is (Anselin 1995). Some clusters of units may exhibit positive autocorrelation while some may exhibit negative autocorrelation and a third group of units may have no autocorrelation at all. Global autocorrelation does not account for this heterogeneity and provides only one unified test statistic for global spatial autocorrelation. LISA, however, estimates localized spatial autocorrelation. Only calculating and taking into account global autocorrelation assumes that spatial autocorrelation is homogeneous throughout the distribution of spatial units. This assumption may not hold, so a local test for autocorrelation should always be used. Further, even if our variable does not exhibit global autocorrelation or clustering, we can use LISA to find possible localized clusters (Anselin 1995). The exact tests for global and local autocorrelation are discussed next.\n\n\nGlobal Morans I Test\nThe first test that we consider is Moran’s \\(I\\) test. Moran’s \\(I\\) test, created by Australian statistician P. A. P. Moran is a formal test for spatial autocorrelation (Moran 1950). Global Moran’s \\(I\\) statistic measures spatial autocorrelation simultaneously based on both variable locations and variable values (Darmofal 2015). Given a set of variables and an associated attribute, Moran’s \\(I\\) indicates if the pattern of spatial autocorrelation is clustered, dispersed, or random (Darmofal 2015).\nThe null hypothesis for this test is that there is random disturbances, or no spatial autocorrelation, and a statistically significant Moran’s \\(I\\) estimate rejects that null hypothesis. As an introductory statistics refresher, \\(p\\)-values are numerical approximations of the area under the curve for a known distribution, limited by the test statistic. In other words, \\(p\\)-values for Moran’s \\(I\\) statistic tell us the probability that we would have seen the extremity of the degree of autocorrelation that we have seen given there was truly no autocorrelation. Similar to standard OLS models, a \\(p\\)-value \\(<.05\\) is evidence that we can reject the null hypothesis that there is no spatial autocorrelation. While this is the conventional confidence level, researchers can also consider \\(p<.01\\) or \\(p<.001\\) confidence levels to be even more confident that there is, in fact, spatial autocorrelation. Usually we want to see a \\(p\\)-value to be as low as possible so that we can be more confident that are result is not due to random chance, but, with testing for spatial autocorrelation, this is perhaps a mistake. We should try to be as aware of possible spatial autocorrelation as possible, meaning we may want to have a confidence level closer to \\(p<.1\\). Setting the threshold \\(p\\)-value higher means that we are more likely to reject the null hypothesis for spatial autocorrelation and more likely to choose to estimate some type of spatial regression model to account for even modestly autocorrelated residuals.\nMoran’s Global \\(I\\) is calculated based on a weighted matrix with unit \\(i\\) and neighbor \\(j\\) (Darmofal 2015). Similarities between units \\(i\\) and \\(j\\) are calculated as the product of the differences between \\(y_i\\) and \\(y_j\\) with the overall mean (Darmofal 2015).\n\\[I=\\frac{N}{W}\\frac{\\sum_i^{N}\\sum_j^{N}w_{ij}(y_i-\\bar{y})(y_j-\\bar{y})}{\\sum_i^{N}(y_i-\\bar{y})}\\]\n\n\\(N\\) is the number of units indexed by \\(i\\) and \\(j\\)\n\\(y\\) is the variable of interest\n\\(\\bar{y}\\) is the average of the variable of interest\n\\(w_{ij}\\) is a matrix of spatial weights with zeroes on the diagonal\n\\(W\\) is the sum of all \\(w_{ij}\\) such that \\({W=\\sum _{i=1}^{N}\\sum _{j=1}^{N}{w_{ij}}}\\)\n\nDefining the exact weights matrix is vital to calculating Moran’s \\(I\\) because the value of \\(I\\) depends on the assumptions built into the spatial weights matrix \\(w_{ij}\\) (Darmofal 2015). The spatial weights matrix constrains the number of neighbors being considered and weights appropriately based on those constraints. In other words, we expect a unit, \\(i\\)’s closest neighbors to be the most similar to \\(i\\), while further distant neighbors may not be at all related.\nThere are several methods that can be used to assign weights. The most basic approach to weighting is to assign a weight of 1 if neighbors are nearby and a weight of 0 otherwise (DGES 2022). Another, slightly more advanced method, is to assign weights based on a \\(k\\) nearest neighbors approach where the \\(k\\) nearest neighbors receive a weight of 1 and 0 otherwise (DGES 2022). The decision of weighting is important because the estimate, and resulting \\(p\\)-value of Moran’s \\(I\\) statistic is heavily dependent upon weighting. This means our inference of whether or not our data has spatial autocorrelation is dependent, partly, upon how we choose to weight similarities and differences between neighbors. library(spdep) makes this process computationally easy and offers functions to weight based on each option enumerated above. spdep::poly2nb() evaluates the spatial distribution of a variable to estimate how similar a unit, \\(i\\) is to their neighbor, \\(j\\). spdep::nb2listw() then constructs the weights based on these estimated similarities. Each function takes specific arguments concerning how to weight, how much to weight nearby neighbors, and whether to weight far away neighbors as a low number or zero. For the \\(k\\) nearest neighbors approach, spdep::knearneigh() and spdep::knn2nb() can construct the weights for neighbors, only if you have point data.\nCalculating Moran’s \\(I\\) statistic by hand is a quite cumbersome process, so we focus on the function spdep::moran.test() which takes the result of the above two functions and returns the estimate of \\(I\\) and the resulting \\(p\\)-value. The sign of the estimate tells us the direction of autocorrelation and the absolute value tells us the degree. If the estimate of \\(I\\) is positive, spatial autocorrelation is positive and if the estimate of \\(I\\) is negative, spatial autocorrelation is negative. The higher the absolute value of \\(I\\), the more severe the degree of autocorrelation. Importantly, global Moran’s \\(I\\) is bounded by \\([-1,1]\\) with -1 being perfect negative autocorrelation and 1 being perfect positive autocorrelation (Darmofal 2015). If the resulting \\(p\\)-value is \\(p<.05\\), we can reject the null hypothesis that there is no spatial autocorrelation. Once we have evidence that our dependent variable exhibits a statistically significant degree of spatial autocorrelation, we can, and should, consider utilizing a spatial regression method.\n\n\nLocal Spatial Autocorrelation (LISA)\nOne weakness of Moran’s global \\(I\\) is that it is assumes homogeneity in spatial autocorrelation. The global \\(I\\) statistic will not tell us if different clusters of neighbors are autocorrelated significantly differently than other clusters of neighbors, resulting in an estimate of \\(I\\) and its \\(p\\)-value not accurately characterizing localized spatial autocorrelation. For a better test statistic, we turn to LISA (Anselin 1995), or local indicators of spatial association which calculates Moran’s \\(I\\) for each individual spatial unit based on the following formula:\n\\[I=\\sum_{i=1}^{N}{\\frac{I_{i}}{N}}\\]\nWhere:\n\n\\(I_i\\) is the local Moran’s I statistics and \\(N\\) is the number of spatial units.\n\nWe use LISA when we believe that the assumption that autocorrelation is constant across all spatial units is not accurate for our data. The resulting measure identifies localized clusters of spatial autocorrelation (Anselin 1995). Beyond using local Moran’s \\(I\\) when the homogeneity assumption is violated, there are several benefits of calculating LISA. When we are thinking about local spatial autocorrelation, there are four different types of cluster relationships (Anselin 2020) that only LISA will show: High-High, or units that have high values of a variable with neighbors that also have high values; High-Low, or units that have high values of a variable with neighbors that have low values; Low-High, or units that have low values of a variable with neighbors that have high values; and Low-Low, or units that have low values of a variable with neighbors that also have low values. Local Moran’s \\(I\\), calculated with spdep::localmoran() can identify these different types of clusters, map them, and better characterize the spatial autocorrelation that we are seeing in our variable. Local Moran’s \\(I\\) can also be used to identify local clusters and outliers that are surrounded by opposite values (Anselin 1995).\nJust as library(spdep) makes calculating global autocorrelation straightforward and easy, LISA is easily calculated with localmoran(). As shown in the applied section in the fourth chapter, we can visualize local autocorrelation easily with library(ggplot2). The main goal of utilizing methods to calculate LISA is to determine the best choice of model selection, as not all spatial models account for localized spatial autocorrelation."
  },
  {
    "objectID": "chapter2.html#intuition-and-simulation",
    "href": "chapter2.html#intuition-and-simulation",
    "title": "Spatial Autocorrelation",
    "section": "Intuition and Simulation",
    "text": "Intuition and Simulation\nHere we see three renditions with different distributions of outcome variables in a 5x5 matrix of grid cells.\nThe first matrix has two clear clusters of data. Another way to look at it in the context of discussion above is - the neighbors have values very similar to each other in the two parts of the matrix. Hence, we see a high positive value of Global Moran as given in above the figure.\nSimilarly, in the second figure, we see that all of the immediate neighbors (that is those that share boundary with a cell), have values different from the cell under observation. Consequently, we see a global Moran’s \\(I\\) statistic of -1.\nIn the third figure we have a bit more random distribution of values. Thus, we see a global Moran’s \\(I\\) statistic close to 0.\n\n\n\n\n\nTo understand the concept of Local Moran’s \\(I\\), we simulate a different matrix grid. This time, the outcome variable is drawn from a continuous multivariate normal distribution.\n\n# The code for this section is adapted from: https://rpubs.com/jguelat/autocorr\n\n#Define function to draw random samples from a multivariate normal\n# distribution\nrmvn <- function(n, mu = 0, V = matrix(1)) {\n  p <- length(mu)\n  if (any(is.na(match(dim(V), p)))) \n    stop(\"Dimension problem!\")\n  D <- chol(V)\n  t(matrix(rnorm(n * p), ncol = p) %*% D + rep(mu, rep(n, p)))\n}\n\n# Set up a square lattice region\nsimgrid <- expand.grid(1:20, 1:20)\nn <- nrow(simgrid)\n\n# Set up distance matrix\ndistance <- as.matrix(dist(simgrid))\n# Generate random variable\nphi <- 0.05\nX <- rmvn(1, rep(0, n), exp(-phi * distance))\n\n# Visualize results\nXraster <- rasterFromXYZ(cbind(simgrid[, 1:2] - 0.5, X))\npar(mfrow = c(1, 2))\n#plot(1:100, exp(-phi * 1:100), type = \"l\", xlab = \"Distance\", ylab = \"Correlation\")\nplot(Xraster, main = \"Distribution\")\nf <- matrix(c(0,1,0,1,0,1,0,1,0), nrow=3)\nplot(MoranLocal(Xraster, w=f), main = \"Local Moran I\")\n\n\n\n\nThe plot on the left is the distribution of a continuous random variable on a 20x20 matrix. The greener the cell the higher the value.\nThe plot on the right is a matrix of the same dimension showing Local Moran’s \\(I\\) for each cell. While the interpretation is a bit tricky, we can use the heuristic that the greener the cell, the more similar the immediate boundary sharing neighbors."
  },
  {
    "objectID": "chapter2.html#a-naive-model-of-spatial-data",
    "href": "chapter2.html#a-naive-model-of-spatial-data",
    "title": "Spatial Autocorrelation",
    "section": "A Naive Model of Spatial Data",
    "text": "A Naive Model of Spatial Data\nNow that we have presented spatial autocorrelation and the tests used to detect it, let’s discuss why OLS is inappropriate to use when modeling spatial relationships. Consider the basic multiple regression model:\n\\[Y_{i}=\\alpha + \\beta\\ X_{i} + \\delta \\Lambda' + \\nu_{i}\\]\nIn this model, we have our dependent variable, or outcome concept, \\(Y_i\\), the intercept, \\(\\alpha\\), or the average of \\(Y\\) when all predictor variables are 0, \\(\\beta\\), the coefficient on our main predictor variable, \\(X_i\\), \\(\\delta \\Lambda '\\), a term for the matrix of covariates and their estimated effects on our dependent variable, and \\(\\nu_i\\), the error term of our model. Knowledge of the properties of OLS is assumed as a prerequisite and a more thorough examination of the Gauss-Markov assumptions is outside the scope of this guide on spatial regression, but, as a refresher, we present the Gauss-Markov assumptions for OLS to be the best linear unbiased estimator below (Greene 2018).\n\nAssumption I: Linearity\n\n\\(y=X\\beta+\\epsilon\\)\n\nAssumption II: Full Rank\n\n\\(X\\) is a \\(n\\times k\\) matrix with rank \\(k\\)\n\nAssumption III: Exogeneity\n\n\\(E[\\epsilon_{i}|X]=0\\)\n\nAssumption IV: Homoscedasticity and No Autocorrelation\n\n\\(E[e_ie_j|X]=0\\)\n\nAssumption V: Data Generating Process\n\n\\(X\\) may be fixed or random\n\nAssumption VI: Errors Normally Distributed\n\n\\(\\epsilon | X \\sim N[0, \\sigma^2I]\\)\n\n\nWhile its important all of these assumptions are met, the key assumption to notice for our purposes here is Assumption IV: Homoscedasticity and No Autocorrelation. In order for OLS to be the best linear unbiased estimator (BLUE), the errors must have constant variance and not be correlated with one another. Errors that do not have constant variance are called heteroscedastic errors. Autocorrelation refers to errors being correlated with one another. In the time series context, as mentioned above, autocorrelation refers to when the errors in \\(t_1\\) are correlated with the errors in \\(t_{-1}\\) (Bailey 2021). This is a problem for spatial units as well. As Waldo Tobler (1970) says, “everything is related to everything else, but near things are more related than distant things.” This means that we can expect clusters in our distribution of our variables of interest across our spatial units. If our data exhibits this type of clustering, we violate at least Assumption IV and possibly a few others.\nFor example, look at the Brooklyn maps in chapter one. The northeast of Brooklyn is a cluster of college educated New Yorkers, suggesting some degree of spatial autocorrelation. When our data is spatially autocorrelated, as long as the other assumptions are not violated, OLS will give us unbiased estimates of \\(\\beta\\), but our standard errors will be inaccurate due to the no autocorrelation assumption being broken. Using a naive OLS model in this way will lead to possible incorrect statistical inference. More specifically, \\(\\nu_i\\) in the model above contains the spatial autocorrelation between units \\(i\\) and \\(j\\) that make OLS inappropriate for estimating a relationship between \\(X_i\\) and \\(Y_i\\). The resulting correlation of the error term must be obviated before we can estimate correct standard errors.\nStaking ones academic reputation or making policy decisions based on a naive estimate of a spatial relationship risks a researcher losing credibility due to inaccuracy. Further, false inference could lead a city government to build a new police station in a sub-optimal location based on a poorly estimated spatial model. Spurious relationships are also likely to be detected because both the dependent and the independent variables in the model are correlated spatially. Due to the risks of inaccuracy, it is vitally important that researchers do not utilize basic OLS models when estimating a relationship that exhibits spatial autocorrelation. Spatial autocorrelation, and spatial regression more broadly, is largely an issue of ensuring accuracy in statistical inference made from our model results.\nThe goal of this section was to introduce the concepts of spatial autocorrelation, the tests to detect spatial autocorrelation, review the Gauss-Markov assumptions, and show why researchers need to be careful when estimating spatial relationships that exhibit spatial autocorrelation. We now turn to models that can account for spatial autocorrelation."
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "Modeling Spatial Relationships",
    "section": "",
    "text": "We have discussed the previous chapters why OLS is inappropriate to use when variables exhibit spatial autocorrelation. We have also covered how to detect if a variable exhibits spatial autocorrelation. This chapter covers the next and most significant step in the spatial regression process: estimating the actual spatial regression models. At this point, a researcher should have visualized their spatial data and tested for spatial autocorrelation and now must make the decision of model selection. The tools discussed in this section will equip researchers to estimate a number of candidate models and select the one that best accounts for the type and degree of spatial autocorrelation in their data. We will cover the two dominant models used to estimate spatial regressions: the spatial error model and the spatial lag model. Both models have analogs, like most aspects of spatial regression, to time series analysis. By the end of this chapter, readers will be comfortable with the theoretical underpinnings and modeling techniques for both spatial error and spatial lag models. We will also cover more complex modeling choices. These models, and by extension this chapter, are the core and most important aspect of spatial regression. By following the steps discussed here, and shown in a detailed applied context in chapter four, researchers can estimate models with confidence that their standard errors are accurate and any and all spatial autocorrelation is purged from their model’s error term.\nThe main problem when analyzing spatial data is that residuals have a high probability of being autocorrelated due to the similarity of nearby neighbors. For example, we can expect census tracts nearby one another in Los Angeles, CA to have more similar crime rates than census tracts across the city from one another. We need a specification that accounts for spatial autocorrelation, lest our standard errors will be incorrect. We have two main options when it comes to modeling choices that handle global spatial autocorrelation: spatial error models and spatial lag models. We begin first by discussing spatial error models."
  },
  {
    "objectID": "chapter3.html#spatial-error-models",
    "href": "chapter3.html#spatial-error-models",
    "title": "Modeling Spatial Relationships",
    "section": "Spatial Error Models",
    "text": "Spatial Error Models\nAs we have said many times, the main issue with modeling spatially correlated variables is that our standard errors will be incorrect. Spatial error models account for the spatial autocorrelation in our residuals, giving us more accurate standard errors (Hurtado 2016 and DGES 2022). The central theory of spatial error models is that spatial autocorrelation can be modeled as a weighted mean of the residuals of a unit’s neighbors (DGES 2022). Spatial error models specifically model the spatial autocorrelation in the error term. Once we have modeled the spatial autocorrelation in our error term, the new error term is free of autocorrelation. In spatial error models, we weight based on the errors of an observation’s neighbors, giving us the following model:\n\\[Y_{i}=\\alpha + \\beta\\ X_{i} + \\delta \\Lambda' + \\nu_{i}\\] \\[\\nu_i=\\lambda_{Err}\\ W_i\\nu_i + \\epsilon_i\\]\nWhere: - \\(lamba\\) is the spatial constant - \\(W\\) is the weighted error of a unit’s neighbors - \\(\\beta\\ X_i\\) is our main independent variable its coefficient - \\(\\delta \\Lambda'\\) is a matrix of covariates and a vector of their coefficients - \\(\\nu\\) is the spatially autocorrelated errors - \\(\\epsilon\\) is the now purged of spatial autocorrelation error term\nThe key takeaway from the equations above is that once we weight the neighbor’s residuals, our new error term, \\(\\epsilon_i\\), is purged of spatial autocorrelation. Now we can move forward estimating our spatial relationship and confidently know that spatial autocorrelation will not be affecting our standard errors. We should also notice a change, sometimes slight, in our variables’ standard errors, t-values, and \\(p\\)-values compared to OLS. Using spatialreg::errorsarlm(), which takes arguments for a model and weights, estimates a spatial error model in R. We can see evidence that there is, in fact, spatial autocorrelation in our models by looking at the estimate of \\(lambda\\) that is given when running summary() on our model. If the \\(p\\)-value of \\(lambda\\) is statistically significant, there was global spatial autocorrelation that has now been accounted for.\nAnother method to handle globally spatial autocorrelated data is a spatial lag model."
  },
  {
    "objectID": "chapter3.html#spatial-lag-models",
    "href": "chapter3.html#spatial-lag-models",
    "title": "Modeling Spatial Relationships",
    "section": "Spatial Lag Models",
    "text": "Spatial Lag Models\nAs opposed to spatial error models that model the part of the error term that is spatially autocorrelated, in a spatial lag model, we include a lagged term as an independent variable, similar to time series lagged DV models. A spatial lag variable averages the weighted neighboring values of a unit, \\(i\\). Spatial lag models compare \\(i\\)’s values with its neighbors values (DGES 2022). The resulting weighted matrix defines what units are \\(i\\)’s neighbors and how much to weight them. We can use standardized weights from library(spdep). Usually, the unit at the center of its defined list of neighbors is not included in the definition of neighbors and the weight of that unit is set to zero. The weight matrix is the same as the weight matrix for global Moran’s \\(I\\) test.\nA spatial lag model looks like:\n\\[Y_i=\\rho_{lag}\\ W_iY_i + \\beta\\ X_i + \\delta \\Lambda' + \\epsilon_i\\]\nWhere:\n\n\\(Y_i\\) is our dependent variable\n\\(\\rho_{lag}\\) is the degree of autocorrelation\n\\(W_i\\) is an observations weighted by its neighbors\n\\(\\beta\\ X_i\\) is our main independent variable its coefficient\n\\(\\delta \\Lambda'\\) is a matrix of covariates and a vector of their coefficients\n\\(\\epsilon_i\\) is the now-purged of autocorrelation error term\n\nIncluding \\(\\rho_{lag}\\) as an independent variable accounts for the spatial autocorrelation in \\(\\epsilon_i\\). Similar to the result of the spatial error model above, our new error term, \\(\\epsilon_i\\) is entirely purged of spatial autocorrelation. In R, we use spatialreg::lagsarlm() for estimating spatial lag models. The resulting output will give us coefficient estimates, standard errors, t-values, and \\(p\\)-values for each variable in our model. We can also check, after running this model, that there was, in fact, spatial autocorrelation by examining the summary output with summary(). If \\(\\rho\\) is statistically significant, then there was global spatial autocorrelation that has now been accounted for.\nNow, when we estimate our main model, our standard errors will be more accurate. We can use a goodness-of-fit metric like \\(R^2\\) values, Akiake Information Criterion, or another metric of choice for model selection between OLS, spatial lag, and spatial error models. While using either spatial lag or spatial error models is always preferable to relying on basic OLS when you have spatially autocorrelated data, it is a good idea to present results from all three models to show readers that your results are robust to differing model specifications. The main weakness of both spatial error and spatial lag models is that they do not account for localized spatial autocorrelation and operate under the same assumption of global Moran’s \\(I\\): spatial autocorrelation is homogeneous across space. This assumption is often violated when working with actual data from the “real world,” so we need more advanced techniques to handle this."
  },
  {
    "objectID": "chapter3.html#advanced-spatial-regression-modeling",
    "href": "chapter3.html#advanced-spatial-regression-modeling",
    "title": "Modeling Spatial Relationships",
    "section": "Advanced Spatial Regression Modeling",
    "text": "Advanced Spatial Regression Modeling\nSpatial lag and spatial error models are the workhorses of spatial regression, but neither of those models account for localized autocorrelation in spatial units and may be inadequate for handling severe spatial autocorrelation. Understanding these two models equips researchers to reliably model spatial relationships with well-behaving data, but other methods may be needed for severe or heterogeneous spatial autocorrelation. While implementing either a spatial error or a spatial lag model will give researchers far more accurate standard errors, and therefore statistical inferences, than a basic OLS model, we still want to be familiar with more advanced models to better account for the specific type and severity of spatial autocorrelation in our data.\n\nSAC/SARAR Models\nA spatial simultaneous autoregressive model, or a “SAC/SARAR” model combines spatial error and spatial lag models together (DGES 2022). We include both a lagged term and model the spatial autocorrelation in the error term. The most basic SAC/SARAR model looks like:\n\\[Y_i=\\rho_{lag}\\ W_iY_i + \\beta\\ X_i + \\delta \\Lambda' + \\nu_i\\] \\[\\nu_i=\\lambda_{Err}\\ W_i\\nu_i + \\epsilon_i\\]\n\n\\(\\rho_{lag}\\) is the degree of autocorrelation\n\\(W_i\\) is an observations weighted by its neighbors\n\\(\\beta\\ X_i\\) is our main independent variable and its coefficient\n\\(\\delta \\Lambda'\\) is a matrix of covariates and a vector of their coefficients\n\\(\\nu_i\\) is the spatially autocorrelated errors\n\\(lamba\\) is the spatial constant\n\\(W_i\\) is the weighted error of an observations neighbors\n\\(\\epsilon\\) is the now-purged of spatial autocorrelation error term\n\nWe can use spdep::sacsarlm() in R to calculate the maximum likelihood estimation of SAC/SARAR models. When we summarize the results in R, we can check the statistical significance of both \\(\\rho\\) and \\(\\lambda\\). As mentioned above, model selection should depend upon some goodness-of-fit metric. In the applied section we rely on AIC values, but researchers can select the goodness-of-fit metric they believe best suits their analysis. All things equal, the SAC/SARAR model should provide a better fit than either the spatial lag or spatial error models individually. One interesting statistical artifact of SAC/SARAR models to keep in mind is that we can skip the all of the steps above and directly estimate a SAC/SARAR model from the beginning of our analysis as a test for spatial autocorrelation. If both \\(\\rho=0\\) and \\(\\lambda=0\\), then we have no spatial autocorrelation and we can proceed by simply using OLS and conclude that our data is not spatially autocorrelated (Hurtado 2016). We do not even have to consider Moran’s \\(I\\).\nWhile a SAC/SARAR can handle sever autocorrelation, it does not account for local spatial autocorrelation. For that, we need even more advanced methods.\n\n\nGeographically Weighted Regression\nWhen spatial heterogeneity exists in our data, we can still see heteroscedastic and correlated errors even when utilizing a spatial lag, spatial error, or SAC/SARAR models. In these cases, our first choice should be to use a geographically weighted regression model. We can use the tools already discussed in this guide to estimate a geographically weighed regression model easily in R. Of all the models discussed in this chapter, geographically weighted regression models are the most powerful and flexible and allow for the richest results when modeling spatial relationships.\nGeographically Weighted Regression (GWR) is most prominently used in geography and public health scholarship, but it has applications to the social sciences, namely when our data exhibits heterogeneous spatial autocorrelation. GWR is a method of spatial regression that accounts for non-stationary variables and models local spatial relationships (DGES 2022). GWR models are an expansion of OLS models that allow the relationship between our independent and dependent variables to vary depending upon location. GWR estimates local models by fitting a regression model to every spatial unit in the dataset. GWR estimates separate models by including the dependent and independent variables of the units falling either within the neighborhood of each unit or the \\(k\\) nearest neighbors of each unit. The model results are not single point estimates, but distributions of point estimates that can be summarized and visualized in engaging ways. Accordingly, GWR runs many regression models, estimating a different but related regression equation for each spatial unit, so the computational time can be considerably longer than with the other methods mentioned above.\nRecall from introductory statistics that a basic linear regression model is:\n\\[Y_{i}=\\alpha + \\beta_1\\ X_{1i} + \\beta_2\\ X_{2i}\\ +...+\\  \\beta_n\\ X_{ni} + \\epsilon_{i}\\]\nWhere:\n\\[\\beta=(X'X)^{-1}X'y\\]\nIn GWR models, \\(\\beta\\) is calculated as:\n\\[\\beta=(X'W_iX)^{-1}X'W_iy\\]\nWhere \\(W_i\\) is the weight matrix, discussed several times before, that weights nearby units more than distant units. Now the \\(\\hat{\\beta}\\) estimator is accounting for spatial autocorrelation.\nGWR models provide a powerful tool to handle heterogeneous spatial autocorrelation. GWR is, however, not without its limitations and it requires the use of different packages than the models above. The main limitation of GWR is that you need a high number of spatial units, or else the weights may over weight some units and GWR can also not accommodate multipoint data. There are further problems with multicollinearity that can arise due to the estimation procedure (Wheeler & Tiefelsdorf 2005). You can estimate a GWR model in R with library(GWmodel), but the process takes several more steps than the spatial error, spatial lag or SAC/SARAR models, and, depending on the number of independent variables and spatial units, can take a long time to run.\nTo estimate a GWR model, we begin by estimating the optimal bandwidth to define local neighborhoods of each unit in our analysis with GWmodel::bw.gwr(). We can define our neighborhoods with the bandwidth argument. R uses a first set of basic OLS models to determine the best bandwidth, this is called the “adaptive” approach. Our results are heavily dependent upon our choice in bandwidth, so it is best to use the default values for gwr.baisc(), which is an adaptive approach (Fotheringham et al. 2002 & Goovaerts 2008). Once we have estimated our optimal bandwidth, we can run a GWR model with GWmodel::gwr.basic(). Now we will have our regression results accounting for heterogeneity in our data’s spatial autocorrelation and we can map localized coefficients and goodness-of-fit metrics (Mennis 2006).\n\n\nBayesian Hierarchical Spatial Models, SARAR, GS2SLS, and the Spatial Durbin Model\nWhile we have covered the most common and flexible approaches to estimating spatial relationships, there are numerous other models that we can consider. Many of the techniques described below require outside software such as Stata or have asymptotic behavior that has not been formally proved yet. So, while we do not go into detail about any of the following models, we present them quickly to give readers an idea about the current literature revolving around estimating spatial relationships in econometrics, statistics, & public health that may be of interest to researchers looking to innovate in the field of spatial regression.\nThe first additional model we consider is the Bayesian hierarchical spatial model. The Bayesian hierarchical model relies on the use of WinBUGs or GeoBugs. Bayesian hierarchical spatial models have become quite widely used in epidemiology and public health departments, but is not used much yet in the social sciences. The key strength of Bayesian hierarchical models is that they model the complex levels inherent in spatial data and flexibly model different types of spatial autocorrelation. Since they are Bayesian models, they offer an approach to spatial regression consistent with the Bayesian philosophical perspective of statistics, not the frequentist paradigm inherent in other models discussed here. Similar to GWR models, Bayesian hierarchical models provide an estimate for each spatial unit, but they further provide an estimate for different levels in spatial data. This is especially useful when researchers are working with multi-level spatial data (Zhu et al. 2006). Another key strength is that Bayesian models allow researchers to incorporate prior beliefs about the spatial distribution and autocorrelation of the data in the model (Zhu et al. 2006). Zhu et al. (2006) overview this technique and apply it to analyzing the relationship between areas with high drug use and violent crime.\nThe second and third additional models that we consider have related uses. Our presentation here follows Hurtado’s (2016) lecture notes and Jin & Lee (2013). GS2SLS, or the generalized spatial two-stage least squares estimator, applies the logic of instrumental variables to spatial data analysis to purge the spatial autocorrelation from the error term. GS2SLS can be implemented in R with spatialreg::stsls(). GS2SLS models work with researchers first obtaining consistent estimates of \\(\\beta\\) and \\(\\lambda\\) from a Spatial AutoRegressive with additional AutoRegressive error structure, or SARAR model. SARAR models are similar to SAC/SARAR models and are the most basic way to account for spatial autocorrelation. A SARAR model looks like:\n\\[Y_i = \\lambda WY_i + \\beta_1X_i + W\\beta_2X_i + u\\]\nWhere:\n\\[u = \\rho Wu + \\epsilon_i\\]\nThe next step in GS2SLS is to use the consistent estimates of \\(\\lambda\\) to obtain an estimate of \\(u\\). With the estimate of \\(u\\), we can then obtain an estimate of \\(\\rho\\). Once we have \\(\\rho\\), we can create a \\(\\rho\\) transformed equation of our main model and then proceed normally with a 2SLS model. Our result should now be free of spatial autocorrelation (Jin & Lee 2013).\nThe final additional model that we will cover briefly is the Spatial Durbin model (SDM) which is strikingly similar to the SARAR model. Our discussion here follows the presentation of Spatial Durbin models in Eilers (2019). SDM includes both a lagged dependent and independent variable in the main model. Both a \\(WY_i\\) and a \\(WX_i\\) term are included. While geographically weighted regression methods are preferred by public health scholars, SDM is one of the most common methods in econometrics. SDM looks like:\n\\[Y_i = \\lambda WY_i + \\beta_1X_i + W\\beta_2X_i + \\rho Wu_i + \\epsilon_i\\]"
  },
  {
    "objectID": "chapter3.html#concluding-remarks-on-modeling-spatial-relationships",
    "href": "chapter3.html#concluding-remarks-on-modeling-spatial-relationships",
    "title": "Modeling Spatial Relationships",
    "section": "Concluding Remarks on Modeling Spatial Relationships",
    "text": "Concluding Remarks on Modeling Spatial Relationships\nAs one final note, we have covered several types of models but many more exist. It is always a good idea to run an analysis on a series of different models to ensure consistent results and robustness of findings across many model specifications. Model selection for spatial regression can be decided by comparing psudeo-\\(R^2\\)s, Akiake Information Criterions (\\(AIC\\)), or another goodness-of-fit statistics. An applied example of this is discussed in the next chapter.\nThis chapter introduced the core models needed to implement spatial regression and overviewed more advanced methods when spatial lag and spatial error models still exhibit some degree of spatial autocorrelation. In the next chapter, we leave the ivory tower of well-behaving statistical theory behind and enter the basement of data analysis to conduct a full spatial study of the effect of college education rates on poverty in Brooklyn, New York."
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "",
    "text": "Up until now, we have not worked with applied data. We have only presented theory, equations, and R commands much like a “cookbook.” This chapter presents a self-contained systematic step-by-step analysis of the influence of college education rates on poverty in order to showcase each step of the spatial analysis process. We return to our maps from the first chapter to predict the type of spatial autocorrelation we should expect; use Moran’s \\(I\\) and LISA methods to detect global and local spatial autocorrelation; run several different spatial models and use goodness-of-fit metrics to select the “best” approach; display the model results alongside OLS results; and include a series of additional maps that better illustrate each concept mentioned throughout this short book. By the end of this chapter, readers will be completely comfortable implementing spatial regression models, interpreting their output, visualizing their results, and undertaking a complete spatial analysis."
  },
  {
    "objectID": "chapter4.html#exploratory-spatial-data-analysis",
    "href": "chapter4.html#exploratory-spatial-data-analysis",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "Exploratory Spatial Data Analysis",
    "text": "Exploratory Spatial Data Analysis\nRecall that we are motivated by the question of whether or not a college education can lift people out of poverty. We started our analysis by mapping our variables of interest. These maps can be found below.\n\n## Redisplaying the Maps\nm1 + m2\n\n\n\n\nTo recap what we covered in the introduction, these maps depict the distribution of poverty and college educated residents across Brooklyn. The maps provide insights into the areas of Brooklyn that are higher educated and the areas that are struggling economically. We can see that the high values of poverty rates and the high rates of bachelor degree holders are clustered in different parts of the city. The maps appear to be almost flipped images of one another with the high poverty areas being the low educational attainment areas, and vice versa. This clsutering is conjectural evidence in support of positive autocorrelation. While there is certainly contextual evidence that there exists a relationship between education and poverty, as social scientists, we are interested in concrete evidence. To conduct a more rigorous analysis of the relationship between poverty and college education, we should implement some form of regression model, but first we should run tests for spatial autocorrelation."
  },
  {
    "objectID": "chapter4.html#applied-detection-of-spatial-autocorrelation",
    "href": "chapter4.html#applied-detection-of-spatial-autocorrelation",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "Applied Detection of Spatial Autocorrelation",
    "text": "Applied Detection of Spatial Autocorrelation\nWhenever we are working with spatial data, we should run some tests for spatial autocorrelation to make sure that our errors are uncorrelated, so that we can be more confident that we are not violating the Gauss-Markov assumptions. We can implement a test for global spatial autocorrelation with Moran’s \\(I\\). The code is given below:\n\n## Moran' I test\n## Testing for Global Autocorrelation\nbrook_shape %>% \n  poly2nb(c('cartodb_id')) %>% ## identifying spatial units for the weight matrix\n  nb2listw(zero.policy = TRUE) %>% ## allowing distant weights to be zero\n  moran.test(brook_shape$poverty_rate, ., zero.policy = TRUE) ## running moran test\n\n\n    Moran I test under randomisation\n\ndata:  brook_shape$poverty_rate  \nweights: .    \n\nMoran I statistic standard deviate = 24.778, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5254679040     -0.0013368984      0.0004520155 \n\n\nThe result is a large positive \\(I\\) and an extremely small \\(p\\)-value, leading us to reject the null hypothesis that there is no spatial autocorrelation in poverty rates. The large and positive value of \\(I\\) indicates positive autocorrelation. We can conduct the same test for our main independent variable: percent with bachelor’s degree.\n\n## Moran' I test\n## Testing for Global Autocorrelation\nbrook_shape %>% \n  poly2nb(c('cartodb_id')) %>%\n  nb2listw(zero.policy = TRUE) %>%\n  moran.test(brook_shape$bach_rate, ., zero.policy = TRUE)\n\n\n    Moran I test under randomisation\n\ndata:  brook_shape$bach_rate  \nweights: .    \n\nMoran I statistic standard deviate = 35.912, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.7617824779     -0.0013368984      0.0004515575 \n\n\nMoran’s \\(I\\) is even larger and more positive for the percent bachelor’s degree variable and the \\(p\\)-value is again extremely small, approaching 0. We can interpret this \\(p\\)-value as the probability that we would have gotten an estimate of spatial autocorrelation this severe if there was, in fact, no spatial autocorrelation.\nBoth visually from a close examination of the maps of poverty and college educated residents and formally from a statistical test for spatial autocorrelation, we can see evidence that our variables of interest exhibit spatial autocorrelation.\nAnother tool that we can use to illustrate spatial autocorrelation is the following scatterplot:\n\n## Scatterplot for Spatial Autocorrelation\nbrook_shape %>% \n  poly2nb(c('cartodb_id')) %>%\n  nb2listw(zero.policy = TRUE) %>%\n  moran.plot(brook_shape$poverty_rate, ., zero.policy = TRUE, ## plot command\n                  xlab = \"Poverty Rate\",\n                  ylab = \"Lagged Neighbors' Poverty Rate\")\n\n\n\n\nThe scatterplot above uses unit, \\(i\\)’s poverty rate to predict \\(i\\)’s neighbor’s poverty rate. If the poverty rate of \\(i\\) is a statistically significant predictor of the poverty rate of \\(i\\)’s neighbors, then we have evidence of spatial autocorrelation. We can see a strong positive relationship in the scatterplot above which is consistent with the results of global Moran’s \\(I\\) test above.\nNow that we have evidence for global autocorrelation, we should also test for local autocorrelation. Our first piece of evidence that our data may exhibit local autocorrelation is the number of observations in the scatterplot above that are clustered. The top-right quadrant indicates a High-High spatial autocorrelation relationship, meaning census tracts with high levels of poverty are clustered nearby with other census tracts with high levels of poverty. We can see that there are other clusters, though smaller in number, present in the other quadrants as well. This indicates some degree of heterogeneity in our spatial autocorrelation. To test for LISA more formally and rigorously, we use localmoran(). First, we calculate the weights for each census tract with the code below. We will also use this code to calculate thenweights needed for spatial regression models and this code contains the same commands we used above for pre-processing our data for global Moran’s \\(I\\).\n\n## Creating Weights for Spatial Autocorrelation\nbrook_nb <- brook_shape %>% \n  poly2nb(c('cartodb_id')) %>%\n  nb2listw(zero.policy = TRUE)\n\nWith the weights calculated, we can estimate local Moran’s \\(I\\) for each census tract in our data with spdep::localmoran(). We also allow weights of neighbors to be 0 by setting zero.policy=TRUE and omit NAs.\n\n## Local Moran's I\nlisa <- spdep::localmoran(brook_shape$poverty_rate, brook_nb, \n                          zero.policy = TRUE, na.action = na.omit)\n\nWhen we run localmoran(), R returns a new dataframe with the same number of rows as the input dataframe, but with five new variables that can be used to detect and characterize the local spatial autocorrelation in our data. The variables and their definitions are explained in the table below that we adapted from this guide.\n\n\n\nVariable\nDefinition\n\n\n\n\nIi\nLocal Moran \\(I\\) statistic\n\n\nE.Ii\nExpected Value of Local Moran \\(I\\) statistic\n\n\nVar.Ii\nVariance of Local Moran \\(I\\) statistic\n\n\nZ.Ii\nStandard deviation of local Moran \\(I\\) statistic\n\n\nPr(z > 0)\n\\(p\\)-value of local Moran statistic\n\n\n\nThe goal when analyzing local Moran’s \\(I\\) values is to catalog census tracts depending on if they are a part of a cluster or if they do not have significant local autocorrelation. When \\(I_i\\), which is Moran’s \\(I\\) for census tract \\(i\\), is high it means that there is a cluster around the unit that has similar values either low or high. A low \\(I_i\\) means that there is no cluster and a unit is surrounded by values that are not similar. In other words, we can classify each unit as either being a part of a cluster or an outlier depending on the value of \\(I_i\\). We can further classify each census tract into High-High, High-Low, Low-Low, and Low-High as mentioned previously.\nBefore we can run a formal statistical test for local autocorrelation, we first have to choose a confidence level. While we select the \\(.05\\) level because it is customary, this choice is obviously subjective. The following code defines a confidence level and then takes the mean of the poverty rate.\n\n## Selecting the confidence level\nconfidence_level <- 0.05\n## Mean of poverty\nmean_pov <- mean(brook_shape$poverty_rate)\n\nWith the confidence level chosen and the mean of our dependent variable calculated, we can find each census tract that has statistically significant local spatial autocorrelation. We calculate the classification of local spatial autocorrelation based on three criteria: the statistical significance as determined by the \\(p\\)-value of the local Moran’s \\(I\\) test; the estimated value of the local \\(I\\); and whether or not the census tract’s poverty rate is above, below, or equal to the mean poverty rate. We also assign the character value “Not Significant” to the NAs.\n\n## Calculating LISA\nlisa <- lisa %>%\n  as_tibble() %>%\n  set_colnames(c(\"Ii\",\"E(Ii)\",\"Var(Ii)\",\"Z_Ii\",\"Pr(z > 0)\")) %>%\n  mutate(lisa_pov = case_when(\n  `Pr(z > 0)` > 0.05 ~ \"Not significant\",\n  `Pr(z > 0)` <= 0.05 & Ii >= 0 & brook_shape$poverty_rate >= mean_pov ~ \"HH\",\n  `Pr(z > 0)` <= 0.05 & Ii < 0 & brook_shape$poverty_rate >= mean_pov ~ \"HL\",\n  `Pr(z > 0)` <= 0.05 & Ii >= 0 & brook_shape$poverty_rate < mean_pov ~ \"LL\",\n  `Pr(z > 0)` <= 0.05 & Ii < 0 & brook_shape$poverty_rate < mean_pov ~ \"LH\",\n))\n\n# Joining Type to Spatial Data\nbrook_shape$lisa_pov <- lisa$lisa_pov %>% \n  replace_na(\"Not significant\")\n\nNow, we can plot the classification for each census tract with library(ggplot2).\n\n## Poverty Rate LISA Plot\nlisa.1 <- ggplot(brook_shape) +\n  geom_sf(aes(fill=lisa_pov)) +\n  scale_fill_manual(values = c(\"red\",\"pink\",\"lightblue\", \"blue\",\"NA\"), name=\"Clusters or \\nOutliers\") +\n  labs(title = \"Local Spatial Autocorrelation in Brooklyn's Poverty Rate\") +\n  theme_void()\nlisa.1\n\n\n\n\nEach classification is visually represented in the plot above. White census tracts have no significant spatial autocorrelation, light blue tracts are Low-High, blue tracts are Low-Low, pink tracts are High-Low, and red tracts are High-High. Many of the clusters of high poverty census tracts are the same tracts that we pointed out when looking at our basic maps, but we now have a more rigorous way to detect and visualize clustering and we know the exact type of clustering.\nS far, we have shown several different methods to detect spatial autocorrelation: visually by simply looking at a map of a variable, with global Moran’s \\(I\\), and with local Moran \\(I\\)’s when we suspect there may be heterogeneity in spatial autocorrelation. Now that we know our data exhibits spatial autocorrelation, we turn to methods to model spatial relationships and purge the autocorrelation from the error term of our models that will surely also contain spatial autocorrelation."
  },
  {
    "objectID": "chapter4.html#ols-results",
    "href": "chapter4.html#ols-results",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "OLS Results",
    "text": "OLS Results\nWe can think of a basic OLS model such as is presented below to start.\n\\[poverty_{i}=\\alpha + \\beta\\ bachelor_{i} + \\delta X' + \\nu_{i}\\]\nWhere: - \\(poverty_i\\) is the poverty rate of census tract \\(i\\) in Brooklyn, New York. - \\(\\alpha\\) is the intercept - \\(\\beta\\) is our main coefficient of interest - \\(bachelor_i\\) is the percent of census tract \\(i\\)’s population with a bachelor’s degree - \\(\\delta \\Lambda'\\) is a matrix of covariates and a vector of their coefficients - \\(\\nu_i\\) is the error term which we can test for spatial autocorrelation\nKnowing that our data is spatially autocorrelated, we want to use the techniques discussed in previous chapters to purge our error term of spatial autocorrelation. For illustrative purposes, however, we will first estimate the above OLS model. We take the additional step of logging our independent variables to both linearize their relationship with the \\(poverty\\_rate_i\\) and to ensure our errors will be as symmetrically distributed as possible. We include covariates for the proportion of a census tracts’ population that is a part of minority identity groups such as African-American, Asian, and Hispanic and a variable that indicates labor force participation. Our analysis here is quite basic and only intended for illustrative purposes. The code below runs two models with lm() and saves the results as ols. and ols.2.\n\n## OLS Models\nols.1 <-lm(poverty_rate~log(1 + bach_rate), data=brook_shape)\nols.2 <-lm(poverty_rate~log(1 + bach_rate) + log(1 + popinlabou) +  log(1 + africaninl) + \n             log(1 + asianinlab) + log(1 + hispanicin), data=brook_shape)\n\nWe can then visualize the results with library(stargazer).\n\n\n\nOLS Results\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPoverty Rate\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nBachelor’s Degree\n\n\n-0.670***\n\n\n-0.602***\n\n\n\n\n\n\n(0.034)\n\n\n(0.036)\n\n\n\n\n\n\n\n\n\n\n\n\nLabor Force\n\n\n\n\n0.0004\n\n\n\n\n\n\n\n\n(0.008)\n\n\n\n\n\n\n\n\n\n\n\n\nAfrican-American Pop\n\n\n\n\n-0.002\n\n\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nAsian Pop\n\n\n\n\n-0.005**\n\n\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nHispanic Pop\n\n\n\n\n0.025***\n\n\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.329***\n\n\n0.216***\n\n\n\n\n\n\n(0.007)\n\n\n(0.052)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n749\n\n\n749\n\n\n\n\nR2\n\n\n0.339\n\n\n0.401\n\n\n\n\nAdjusted R2\n\n\n0.338\n\n\n0.397\n\n\n\n\nResidual Std. Error\n\n\n0.105 (df = 747)\n\n\n0.100 (df = 743)\n\n\n\n\nF Statistic\n\n\n383.002*** (df = 1; 747)\n\n\n99.440*** (df = 5; 743)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\nLet’s examine the results. Our null hypothesis is that higher rates of residents with bachelor degrees in a census tract will have no effect on a census tract’s poverty rate. In both the simple regression model and the multiple regression model, the bachelor’s degree variable is statistically significant. Our \\(p\\)-value is extremely close to zero and the 95% confidence interval does not cross zero, providing evidence to reject the null hypothesis that there is no relationship between rates of college education and rates of poverty for census tracts in Brooklyn.\nThese results indicate that the census tracts that have a high level of bachelor’s degree attainment have lower levels of poverty, as evidenced by the negative coefficient on the bachelor’s degree variable. We are modeling an inherently spatial relationship, however, so we should be concerned that spatial autocorrelation is causing our standard errors to be incorrect. Incorrect standard errors could lead us to commit a Type I error, or rejecting the null hypothesis when we should not. We also ran Moran’s \\(I\\) test and found evidence of statistically significant univariate spatial autocorrelation, so we should be concerned about violating the assumptions of OLS. To account for spatial autocorrelation, we can run a spatial error or a spatial lag model. We estimate both and report the results below."
  },
  {
    "objectID": "chapter4.html#spatial-error-results",
    "href": "chapter4.html#spatial-error-results",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "Spatial Error Results",
    "text": "Spatial Error Results\nBefore we can estimate a spatial error or any spatial regression model, we must calculate weights based on the degree of spatial autocorrelation our variables exhibit. The following code detects spatial autocorrelation and creates weights to be used in the spatial error and spatial lag models. We have used this same code just above and we can no re-use it for estimating our models.\n\n## Creating Weights for Spatial Autocorrelation\nbrook_nb <- brook_shape %>% \n  poly2nb(c('cartodb_id')) %>%\n  nb2listw(zero.policy = TRUE)\n\nNow with the weights created, we can estimate the a spatial error model with errorsarlm(). We estimate both a simple and multiple regression model with the same control variables that are included above in the OLS model. There are some additional arguments beyond what is required for an OLS models to properly use errorsarlm(). The weights must be included, which we named brook_nb, we need to specify zero.policy=TRUE which allows weights to equal zero, and omit NAs. The spatial error model is:\n\\[poverty\\_rate_{i}=\\alpha + \\beta\\ bach\\_rate_{i} + \\delta X' + \\nu_{i}\\\\ \\nu_i=\\lambda_{Err}\\ W_i\\nu_i + \\epsilon_i\\] The following code estimates the model:\n\n## Spatial Regression Models\nspat_err.1 <- errorsarlm(poverty_rate~log(1 + bach_rate), data=brook_shape,\n                         listw = brook_nb, zero.policy = TRUE, na.action = na.omit)\n\nspat_err.2 <- errorsarlm(poverty_rate~log(1 + bach_rate) + log(1 + popinlabou) +  log(1 + africaninl)+\n                         log(1 + asianinlab) + log(1 + hispanicin), data=brook_shape,\n                         listw = brook_nb, zero.policy = TRUE, na.action = na.omit)\n\n\n\n\nSpatial Error Results\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPoverty Rate\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nBachelor’s Degree\n\n\n-0.740***\n\n\n-0.702***\n\n\n\n\n\n\n(0.048)\n\n\n(0.050)\n\n\n\n\n\n\n\n\n\n\n\n\nLabor Force\n\n\n\n\n-0.003\n\n\n\n\n\n\n\n\n(0.007)\n\n\n\n\n\n\n\n\n\n\n\n\nAfrican-American Pop\n\n\n\n\n0.001\n\n\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nAsian Pop\n\n\n\n\n0.001\n\n\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nHispanic Pop\n\n\n\n\n0.010***\n\n\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.343***\n\n\n0.301***\n\n\n\n\n\n\n(0.012)\n\n\n(0.045)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n749\n\n\n749\n\n\n\n\nLog Likelihood\n\n\n768.430\n\n\n775.380\n\n\n\n\nsigma2\n\n\n0.007\n\n\n0.007\n\n\n\n\nAkaike Inf. Crit.\n\n\n-1,528.860\n\n\n-1,534.759\n\n\n\n\nWald Test (df = 1)\n\n\n346.893***\n\n\n298.453***\n\n\n\n\nLR Test (df = 1)\n\n\n277.613***\n\n\n217.793***\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\nThe statistically significant, \\(p <.01\\), estimate of \\(lambda\\) above indicates that the error term was spatially autoregressive. Regardless of any other metric we can use to evaluate the efficacy of this spatial error model in terms of other model specifications, the fact that we have statistically significant evidence of spatial autocorrelation means that this model is preferable to OLS. To further confirm this, we can compare the models in terms of the Akiake Information Criterion (AIC). AIC is a goodness-of-fit measure that shows how well the model fits the data, and we can use it to select the highest performing model. Lower AIC values are associated with better performing models. The output of errorsarlm() using the summary() function in base R provides information about how well the spatial error model and the OLS performed. The AIC for the spatial error model is -1534.8 and the AIC for the OLS model is -1319, indicating that the spatial error model performed better than OLS.\nIn terms of substantive results, we can see that the percent Asian variable has lost statistical significance. Overall, however, the substantive conclusions from the OLS model are very much the same as the spatial error model. Our main variable of interest, the proportion of residents with a bachelor’s degree, remains statistically significant at the \\(p<.01\\) level and has a similar, yet not exactly the same, coefficient. Importantly, the standard error is now higher, indicating that we were overestimating our certainty in the OLS model."
  },
  {
    "objectID": "chapter4.html#spatial-lag-results",
    "href": "chapter4.html#spatial-lag-results",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "Spatial Lag Results",
    "text": "Spatial Lag Results\nWe can replicate our analysis with the spatial lag model. That model looks like:\n\\[poverty\\_rate_i=\\rho_{lag}\\ W_ipoverty\\_rate_i + \\beta\\ _ibach\\_rate + \\delta X' + \\epsilon_i\\]\nRecall from the previous chapter that we include a weighted lagged dependent variable as an independent variable. We can use nearly the same code as for the spatial error model, expect we substitute lagsarlm() for errorsarlm()\n\nspat_lag.1 <- lagsarlm(poverty_rate~log(1 + bach_rate), data=brook_shape,\n                       listw = brook_nb, zero.policy = TRUE, na.action = na.omit)\n\nspat_lag.2 <- lagsarlm(poverty_rate~log(1 + bach_rate) + log(1 + popinlabou) +  log(1 + africaninl) + \n                           log(1 + asianinlab) + log(1 + hispanicin), data=brook_shape,\n                         listw = brook_nb, zero.policy = TRUE, na.action = na.omit)\n\n\n\n\nSpatial Lag Results\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPoverty Rate\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nBachelor’s Degree\n\n\n-0.394***\n\n\n-0.378***\n\n\n\n\n\n\n(0.038)\n\n\n(0.038)\n\n\n\n\n\n\n\n\n\n\n\n\nLabor Force\n\n\n\n\n-0.004\n\n\n\n\n\n\n\n\n(0.007)\n\n\n\n\n\n\n\n\n\n\n\n\nAfrican-American Pop\n\n\n\n\n-0.001\n\n\n\n\n\n\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\nAsian Pop\n\n\n\n\n-0.002\n\n\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\nHispanic Pop\n\n\n\n\n0.016***\n\n\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.160***\n\n\n0.125***\n\n\n\n\n\n\n(0.014)\n\n\n(0.046)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n749\n\n\n749\n\n\n\n\nLog Likelihood\n\n\n738.477\n\n\n756.824\n\n\n\n\nsigma2\n\n\n0.008\n\n\n0.007\n\n\n\n\nAkaike Inf. Crit.\n\n\n-1,468.953\n\n\n-1,497.648\n\n\n\n\nWald Test (df = 1)\n\n\n242.443***\n\n\n197.383***\n\n\n\n\nLR Test (df = 1)\n\n\n217.706***\n\n\n180.682***\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\nWe can now evaluate the spatial lag model both in terms of how it performs compared to OLS and the spatial error model. The statistical significance of \\(\\rho\\), as indicated by the extremely small \\(p\\)-value, is another sign that our error term in the basic OLS model was autoregressive. This is immediate evidence that the results of this model are more accurate than OLS. We can see that several variables have lost their statistical significance and the coefficient estimate on the percent bachelor’s degree variable is somewhat smaller than before. The standard errors are also higher, indicating a larger degree of uncertainty in our point estimate than basic OLS shows. Turning to our main model selection criteria, the AIC value is lower for the spatial lag model compared to OLS but higher than the AIC in the spatial error model. In sum, the spatial lag model performs better and is preferable to OLS, but the spatial error model is the best performing model overall. For any given research project, authors should present all three models and consider a more advanced spatial regression model to provide further robustness checks and ensure readers that results are consistent across varying model specifications.\nTo that end, we present all three models in the same table for ease of analysis here:\n\n\n\nAll Model Results\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPoverty Rate\n\n\n\n\n\n\nOLS\n\n\nspatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nerror\n\n\nautoregressive\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n\n\nBachelor’s Degree\n\n\n-0.670***\n\n\n-0.602***\n\n\n-0.740***\n\n\n-0.702***\n\n\n-0.394***\n\n\n-0.378***\n\n\n\n\n\n\n(0.034)\n\n\n(0.036)\n\n\n(0.048)\n\n\n(0.050)\n\n\n(0.038)\n\n\n(0.038)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabor Force\n\n\n\n\n0.0004\n\n\n\n\n-0.003\n\n\n\n\n-0.004\n\n\n\n\n\n\n\n\n(0.008)\n\n\n\n\n(0.007)\n\n\n\n\n(0.007)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrican-American Pop\n\n\n\n\n-0.002\n\n\n\n\n0.001\n\n\n\n\n-0.001\n\n\n\n\n\n\n\n\n(0.002)\n\n\n\n\n(0.002)\n\n\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsian Pop\n\n\n\n\n-0.005**\n\n\n\n\n0.001\n\n\n\n\n-0.002\n\n\n\n\n\n\n\n\n(0.002)\n\n\n\n\n(0.002)\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHispanic Pop\n\n\n\n\n0.025***\n\n\n\n\n0.010***\n\n\n\n\n0.016***\n\n\n\n\n\n\n\n\n(0.003)\n\n\n\n\n(0.003)\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.329***\n\n\n0.216***\n\n\n0.343***\n\n\n0.301***\n\n\n0.160***\n\n\n0.125***\n\n\n\n\n\n\n(0.007)\n\n\n(0.052)\n\n\n(0.012)\n\n\n(0.045)\n\n\n(0.014)\n\n\n(0.046)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n749\n\n\n749\n\n\n749\n\n\n749\n\n\n749\n\n\n749\n\n\n\n\nR2\n\n\n0.339\n\n\n0.401\n\n\n\n\n\n\n\n\n\n\n\n\nAdjusted R2\n\n\n0.338\n\n\n0.397\n\n\n\n\n\n\n\n\n\n\n\n\nLog Likelihood\n\n\n\n\n\n\n768.430\n\n\n775.380\n\n\n738.477\n\n\n756.824\n\n\n\n\nsigma2\n\n\n\n\n\n\n0.007\n\n\n0.007\n\n\n0.008\n\n\n0.007\n\n\n\n\nAkaike Inf. Crit.\n\n\n\n\n\n\n-1,528.860\n\n\n-1,534.759\n\n\n-1,468.953\n\n\n-1,497.648\n\n\n\n\nResidual Std. Error\n\n\n0.105 (df = 747)\n\n\n0.100 (df = 743)\n\n\n\n\n\n\n\n\n\n\n\n\nF Statistic\n\n\n383.002*** (df = 1; 747)\n\n\n99.440*** (df = 5; 743)\n\n\n\n\n\n\n\n\n\n\n\n\nWald Test (df = 1)\n\n\n\n\n\n\n346.893***\n\n\n298.453***\n\n\n242.443***\n\n\n197.383***\n\n\n\n\nLR Test (df = 1)\n\n\n\n\n\n\n277.613***\n\n\n217.793***\n\n\n217.706***\n\n\n180.682***\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "chapter4.html#geographically-weighted-regression",
    "href": "chapter4.html#geographically-weighted-regression",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "Geographically Weighted Regression",
    "text": "Geographically Weighted Regression\nFollowing our discussion of the two main workhorse models of spatial regression, the spatial error and the spatial lag models, we talked about several more advanced techniques to estimate spatial relationships that contain a high degree of spatial autocorrelation or have heterogeneity (LISA) in their spatial autocorrelation. We now end our applied section with showing how to implement one of the most common and poweful of the more advanced models: the GWR model. We start by estimating a geographically weighted regression model, compare its performance to the above models, and visualize the results. library(GWmodel) in tandem with library(spdep) provides the tools needed to estimate a geographically weighted regression model. We first need to remove all NAs from the dataset to accurately calculate the weights.\n\n# Remove all the NAs in the dataset\nbrook_shape_no_na <- brook_shape %>% \n  drop_na()\n\nNow that we have removed the NAs, we can calculate the weights for each unit. Remember: GWR models run separate regressions for each spatial unit.\n\n# Estimate an optimal bandwidth\nweights <- GWmodel::bw.gwr(poverty_rate~log(1 + bach_rate) + log(1 + popinlabou) +\n                           log(1 + africaninl) + \n                           log(1 + asianinlab) + log(1 + hispanicin),\n                           data = brook_shape_no_na %>% \n                           sf::as_Spatial(), \n                           approach = 'AICc', kernel = 'bisquare', \n                           adaptive = TRUE)\n\nAdaptive bandwidth (number of nearest neighbours): 470 AICc value: -1481.673 \nAdaptive bandwidth (number of nearest neighbours): 298 AICc value: -1565.594 \nAdaptive bandwidth (number of nearest neighbours): 191 AICc value: -1621.487 \nAdaptive bandwidth (number of nearest neighbours): 125 AICc value: -1658.369 \nAdaptive bandwidth (number of nearest neighbours): 84 AICc value: -1666.586 \nAdaptive bandwidth (number of nearest neighbours): 59 AICc value: -1639.162 \nAdaptive bandwidth (number of nearest neighbours): 100 AICc value: -1667.002 \nAdaptive bandwidth (number of nearest neighbours): 109 AICc value: -1663.021 \nAdaptive bandwidth (number of nearest neighbours): 93 AICc value: -1668.509 \nAdaptive bandwidth (number of nearest neighbours): 90 AICc value: -1668.185 \nAdaptive bandwidth (number of nearest neighbours): 96 AICc value: -1666.886 \nAdaptive bandwidth (number of nearest neighbours): 92 AICc value: -1668.245 \nAdaptive bandwidth (number of nearest neighbours): 94 AICc value: -1667.665 \nAdaptive bandwidth (number of nearest neighbours): 92 AICc value: -1668.245 \nAdaptive bandwidth (number of nearest neighbours): 93 AICc value: -1668.509 \n\n\nNow that we have the weights calculated, we can run the main GWR model. The code and results to run the model and display the results are below. Unfortunately, we cannot use library(stargazer) to create a nicely formatted table for GWR models due to the nature and complexity of the output.\n\ngwr_model <- GWmodel::gwr.basic(poverty_rate~log(1 + bach_rate) + log(1 + popinlabou) +  \n                                log(1 + africaninl) + \n                                log(1 + asianinlab) + log(1 + hispanicin),\n                                data = brook_shape_no_na %>% \n                                sf::as_Spatial(), \n                                bw = weights, kernel = \"bisquare\", adaptive = TRUE)\nprint(gwr_model)\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-12 12:56:04 \n   Call:\n   GWmodel::gwr.basic(formula = poverty_rate ~ log(1 + bach_rate) + \n    log(1 + popinlabou) + log(1 + africaninl) + log(1 + asianinlab) + \n    log(1 + hispanicin), data = brook_shape_no_na %>% sf::as_Spatial(), \n    bw = weights, kernel = \"bisquare\", adaptive = TRUE)\n\n   Dependent (y) variable:  poverty_rate\n   Independent variables:  bach_rate popinlabou africaninl asianinlab hispanicin\n   Number of data points: 749\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-0.21076 -0.07050 -0.00425  0.05426  0.38188 \n\n   Coefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)          0.2155755  0.0521394   4.135 3.96e-05 ***\n   log(1 + bach_rate)  -0.6015869  0.0360114 -16.705  < 2e-16 ***\n   log(1 + popinlabou)  0.0003792  0.0084544   0.045   0.9642    \n   log(1 + africaninl) -0.0021506  0.0017472  -1.231   0.2187    \n   log(1 + asianinlab) -0.0053706  0.0020896  -2.570   0.0104 *  \n   log(1 + hispanicin)  0.0254460  0.0031623   8.047 3.37e-15 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 0.09978 on 743 degrees of freedom\n   Multiple R-squared: 0.4009\n   Adjusted R-squared: 0.3969 \n   F-statistic: 99.44 on 5 and 743 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 7.397933\n   Sigma(hat): 0.09951646\n   AIC:  -1318.966\n   AICc:  -1318.815\n   BIC:  -1989.304\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidth: 93 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                             Min.    1st Qu.     Median    3rd Qu.   Max.\n   Intercept           -0.3718597  0.1125074  0.2935530  0.4912229 1.1779\n   log(1 + bach_rate)  -2.4709224 -1.1641975 -0.8129934 -0.5343122 0.5593\n   log(1 + popinlabou) -0.2539616 -0.0165205  0.0190715  0.0456808 0.3136\n   log(1 + africaninl) -0.3245012 -0.0152142 -0.0010035  0.0120940 0.1672\n   log(1 + asianinlab) -0.0266489 -0.0030611  0.0015387  0.0068593 0.0312\n   log(1 + hispanicin) -0.0663042 -0.0066083  0.0064852  0.0169869 0.0639\n   ************************Diagnostic information*************************\n   Number of data points: 749 \n   Effective number of parameters (2trace(S) - trace(S'S)): 141.6189 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 607.3811 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): -1668.509 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): -1815.489 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): -1959.431 \n   Residual sum of squares: 3.364251 \n   R-square value:  0.7275574 \n   Adjusted R-square value:  0.6639291 \n\n   ***********************************************************************\n   Program stops at: 2022-12-12 12:56:04 \n\n\nThe output, shown above, gives a wealth of information about regression results, goodness-of-fit metrics, and the more meta information about the estimation procedure. Let’s go over some of it now. We see first the results of the global regression and below, in the second section, we see the results of the GWR. Importantly, the GWR output gives us a range of coefficient estimates since we have conducted an individual regression model for each census tract. The results that we see in the second table are actually more like descriptive statistics of the distribution of estimated coefficients and other regression result metrics. We can also see the more meta information of our GWR models such as the adaptive bandwidth, the kernel function, and the distance metric. We have left all of these values default as suggested by Fotheringham et al. (2002) & Goovaerts (2008).\nIn terms of model selection, the GWR performs better based on AIC values that the other three models that we have estimated, with the AIC for the GWR model being the lowest of all models tested. This result indicates that the GWR models is the best performing model compared to OLS, the spatial lag, and the spatial error models. The adjusted \\(R^2\\) value is also higher in the GWR model as compared to OLS. We also have a higher coefficient to standard error ratio giving us a higher t-value and a more certain point estimate. While the coefficients have not changed drastically from the OLS model results, we know through the numerous techniques to detect and estimate spatial autocorrelation, that our relationship exhibited spatial autoregressive qualities and is now more accurate. We were able to increase the performance of our models, in terms of AIC, by employing GWR, showing the benefits of employing more advanced spatial modeling techniques."
  },
  {
    "objectID": "chapter4.html#mapping-geographically-weighted-regression-results",
    "href": "chapter4.html#mapping-geographically-weighted-regression-results",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "Mapping Geographically Weighted Regression Results",
    "text": "Mapping Geographically Weighted Regression Results\nGWR models offer a key benefit in unit level metrics that allow us to evaluate and visualize our model results on a localized level. Accordingly, an important and powerful aspect of library(GWmodel) is the ability to easily visualize results for each spatial unit. We first need to process our model results into a form that can extract coefficients, standard errors, goodness-of-fit, and other metrics from our GWR model. Specifically, spplot() cannot recognize the names of our variables resulting from the GWR model estimation (DGES 2022), so we need to change the names, which the following code does.\n\n## Processing GWR model for visualization\ngwr_proc_data <- gwr_model$SDF@data\n## Changing Bachelor's Degree Variable Name\ngwr_proc_data$coefbach <- gwr_proc_data$`log(1 + bach_rate)`\n## Calculating p-values\ngwr_proc_data$bachp <- 2*pt(-abs(gwr_proc_data$`log(1 + bach_rate)_TV`), \n                            df = dim(gwr_proc_data)[1] -1)\n## Creating a standard error variable\ngwr_proc_data$sebach <- gwr_proc_data$`log(1 + bach_rate)_SE`\n## Finalizing the preprocessing\nprocessed_gwr_results <- gwr_model$SDF\nprocessed_gwr_results@data <- gwr_proc_data\n\nNow that we have the data processed, we end this section with the visualization of the GWR model results. Since we have estimated a series of regression models, we have resulting metrics for every single census tract that we can create maps with. Perhaps one of the richest ways to visualize heterogeneity in spatial autocorrelation is with the following plots that visualize the distributions of our key model results across each census tract in Brooklyn. Visualizing the GWR models results in this fashion is an incredibly powerful technique to show how deleterious spatial autocorrelation can be and to show what localized spatial autocorrelation looks like. We start with mapping localized \\(R^2\\) values.\n\nspplot(processed_gwr_results, \"Local_R2\", main=\"Local Pseudo R Squared Values\")\n\n\n\n\nMapping the pseudo \\(R^2\\) values for each census tract shows the heterogeneity in model fit depending on the tract. This adds a level of richness to our understanding of our models because we know that our model specification performs better when estimating the effect of college education rates on poverty rates in some census tracts compared to others.\n\nspplot(processed_gwr_results, \"coefbach\", main=\"Bachelor's Degree Estimated Coefficients\")\n\n\n\n\nThe map of estimated coefficients above shows where the effect of college education rates on poverty rates, controlling for all other factors in the model, is the highest and lowest. While our overall coefficient estimate is negative, we can see a cluster in central Brooklyn where the estimated coefficient is positive. This tells us that bachelor’s degree attainment does not always lead to lower levels of poverty and its effect is heterogeneous across spatial units. We can also further see clustering of high and low coefficients in different locations around the borough.\n\nspplot(processed_gwr_results, \"sebach\", main=\"Standard Error of Bachelor's Degree Coefficients\")\n\n\n\n\nThis third map shows the distribution of standard errors across census tracts. We see the lowest standard errors on the entire west side of Brooklyn with the highest standard errors in the northeastern parts of the borough. We can use this map to visualize the distribution of our uncertainty in our estimated coefficients and see where the confidence intervals will be the highest and the lowest. In other words, this is a map of our uncertainty concerning our estimated coefficients.\n\nspplot(processed_gwr_results, \"bachp\", main=\"p-value of Bachelor's Degree Coefficients\")\n\n\n\n\nFinally, we map the distribution of \\(p\\)-values. We can see extremely low \\(p\\)-values, indicating statistical significance, across the majority of the borough with only a small cluster of census tracts where the rate of college education was not a statistically significant predictor of poverty rates.\nIt is an extremely good idea to create these visualizations whenever we run a GWR model so that we can better characterize the full distribution of our model results. Since GWR runs many models, only looking at averaged metrics loses the richness that makes GWR models valuable to researchers."
  },
  {
    "objectID": "chapter4.html#concluding-remarks",
    "href": "chapter4.html#concluding-remarks",
    "title": "Poverty & Education in Brooklyn, New York",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nWe have presented an applied example that shows how to detect and manage spatial autocorrelation. The analysis presented here is not the only way that spatial data can be analyzed, and we only considered one of the more advanced spatial regression models at our disposal. It is up to researchers to make wise decisions about model specification and to be careful in how to control for differing types of spatial autocorrelation."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anselin, Luc. 1995. Local Indicators of Spatial Association — LISA. Geographical Analysis 27: 93–115.\nAnselin, Luc. 2020. Local Spatial Autocorrelation. GeoDa.Link\nBailey, Michael A. 2021. Real Statistics. 2nd edition. Oxford University Press.\nBivand R, Pebesma EJ, Gómez-Rubio V. 2008. Applied spatial data analysis using R. Heidelberg: Springer.\nDarmofal, David. 2006. Spatial Econometrics and Political Science.Link\nDarmofal, David. 2015. Spatial Analysis for the Social Sciences. Cambridge University Press.\nEilers, Lea. 2019. Spatial Autocorrelation and the Spatial Durbin Model. Robust Workshop Lecture Slides. Link\nFotheringham AS, Brundson C, and Charlton M. 2002. Geographically weighted regression: The analysis of spatially varying relationships. West Sussex, England: John Wiley and Sons, Ltd.\nFranzese, R., & Hays, J. 2017. Spatial Econometric Models of Cross-Sectional Interdependence in Political Science Panel and Time-Series-Cross-Section Data. Political Analysis, 15(2), 140-164.\nGoovaerts P. 2008. Geostatitical Analysis of Health Data: State-of-the-Art and Perspectives. Soares A, Pereira MJ, & Dimitrakopolous R (Eds.) Proceedings of the Sixth European Conferences on Geostatistics for Environmental Applications (pp. 3-22).\nGreene, William H. 2018. Econometric Analysis. 8th edition. Pearson Education.\nHurtado, Carlos. 2016. Stationary, Homoscedastic, and Cross-Sectional Spatial Models. Lecture Slides. Link\nJin, Fei and Lung-fei Lee. 2013. Generalized Spatial Two Stage Least Squares Estimation of Spatial Autoregressive Models with Autoregressive Disturbances in the Presence of Endogenous Regressors and Many Instruments. Econometrics. 1(1), 71-114\nMennis J. 2006. Mapping the Results of Geographically Weighted Regression. The Cartographic Journal 43(2): 171-179.\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nMourao, Paulo Reis, Mihaela Bronic, and Branko Stanic. 2020. Discussing the determinants of online budget transparency based on a spatial regression analysis of Croatian cities and municipalities: Do good neighbors make you better?. International Area Studies Review. 1-20.Link\nLin, Tse-Min, Chin-En Wu, and Feng-Yu Lee. 2006. “Neighborhood” Influence on the Formation of National Identity in Taiwan: Spatial Regression with Disjoint Neighborhoods. Political Research Quarterly, 59(1), 35–46.\nLiu Y, Jiang S, Liu Y, Wang R, Li X, Yuan Z, Wang L, and Xue F. (2011) Spatial epidemiology and spatial ecology study of worldwide drug-resistant tuberculosis. International Journal of Health Geographics 10:50.\nTobler W., 1970 A computer movie simulating urban growth in the Detroit region. Economic Geography, 46(Supplement): 234–240.\nShoff C and Yang TC. 2012. Spatially varying predictors of teenage birth rates among counties in the United States. Demographic Research 27(14):377-418.\nWheeler D and Tiefelsdorf M. 2005. Multicollinearity and correlation among local regression coefficients in geographically weighted regression. Journal of Geographical Systems 7(2):161-187.\nYoo D. 2012. Height and death in the Antebellum United States: a view through the lens of geographically weighted regression. Economics and Human Biology 10(1):43-53.\nZhu, L., Gorman, D.M. & Horel, S. 2006. Hierarchical Bayesian spatial models for alcohol availability, drug “hot spots” and violent crime. Int J Health Geogr 5, 54."
  },
  {
    "objectID": "references.html#appendix",
    "href": "references.html#appendix",
    "title": "References",
    "section": "Appendix",
    "text": "Appendix\nIn this section, readers can find additional guides that informed this project and a brief overview of the political science literature that is using spatial regression methods.\nFor more resources that guided this project:\n\nGeographically Weighted Regression\n\nhttps://www.publichealth.columbia.edu/research/population-health-methods/geographically-weighted-regression\n\nRaster & Point Data\n\nhttps://rspatial.org/raster/analysis/8-pointpat.html\n\nSpatial Data Analysis and Visualization with R (cited as DGES)\n\nhttp://www.geo.hunter.cuny.edu/~ssun/R-Spatial/\n\nSpatial Simulation Examples\n\nhttp://gis.humboldt.edu/OLM/r/Spatial%20Analysis%20With%20R.pdf\nhttps://rpubs.com/jguelat/autocorr\n\nSpatial Autocorrelation\n\nhttps://www.r-bloggers.com/2019/07/exploring-spatial-autocorrelation-in-r/\n\nSpatial Weighting\n\nhttps://rpubs.com/erikaaldisa/spatialweights\n\n\nAn Overview of How Spatial Modeling is Applied in Political Science\nAdvancements in spatial modeling have improved measurements for research questions whose theoretical arguments are dependent upon accounting for space and time. In political science and spatial econometrics, models have progressed our understanding of topics such as political geography, political identity, and political opinions. Modeling relationships without accounting for space has led to inaccurate predictions and other estimation errors (Darmofal, 2006). Experts in the field have advocated for moving away from non-spatial OLS regressions to models that account for the ways in which space can impact bias, accuracy in standard errors, and efficiency of estimation (Franzese and Hays, 2017). Lin et al. (2006) used spatial modeling techniques to determine how Taiwanese national identity was shaped through neighborhood influence. Mourao et al. (2020) incorporated spatial models into their analysis to better understand how providing citizens with local budgetary transparency can facilitate legislative transparency and better accountability. These relevant examples highlight the importance of correct model specification and its applications, but spatial methods are not yet popular in political science. One goal of this project, is to increase the usage of spatial regression by political scientists."
  }
]